{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d918096",
   "metadata": {},
   "source": [
    "# Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27d1565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b57c62c",
   "metadata": {},
   "source": [
    "# Importing CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda64fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_csv=input(\"Enter your path to input file\") or \"Input.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3e1f13",
   "metadata": {},
   "source": [
    "# Web Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c05040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The function copies the text from the url, leaving the blacklisted items behind, and saves them in corresponding text files.\n",
    "def CreateText(res,i,f):\n",
    "    #print(keys(res))\n",
    "    html_page = res.text\n",
    "    soup = BeautifulSoup(html_page, 'html.parser')\n",
    "    text = soup.find_all(text=True)\n",
    "\n",
    "    output = ''\n",
    "    blacklist = [\n",
    "        '[document]',\n",
    "        'noscript',\n",
    "        'header',\n",
    "        'html',\n",
    "        'meta',\n",
    "        'head', \n",
    "        'input',\n",
    "        'script', \n",
    "        'style'\n",
    "    # there may be more elements you don't want, such as \"style\", etc.\n",
    "    ]\n",
    "\n",
    "    for t in text:\n",
    "        if t.parent.name not in blacklist:\n",
    "            output += '{} '.format(t)\n",
    "    content_required =  soup.find_all(\"div\", {\"class\": \"td-post-content\"})\n",
    "    for tg in soup.findAll('pre', {\"class\": \"wp-block-preformatted\"}):\n",
    "        tg.decompose()\n",
    "    i=0\n",
    "    for tag in content_required:\n",
    "        lines=tag.getText()\n",
    "        f.write(lines)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07babd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The function goes through every link present in the DataFrame, df, and sends a request to access them.\n",
    "def CollectData(df):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    filepath=input(\"Enter the path to your file\")\n",
    "    for i, row in df.iterrows():\n",
    "        url=row['URL']\n",
    "        filename=filepath+str(i)+\".txt\"\n",
    "        f = open(filename, 'w')\n",
    "        res = requests.get(url,headers=headers)\n",
    "    \n",
    "        CreateText(res,i,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da67c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(path_to_csv)\n",
    "df=df.dropna(axis=0,how='all')\n",
    "df=df.dropna(axis=1,how='all')\n",
    "df[\"URL_ID\"] = df[\"URL_ID\"].astype(int)\n",
    "CollectData(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58385b71",
   "metadata": {},
   "source": [
    "## Function Hub has all the functions needed to complete the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884b0098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FunctionHub(df,text,text_no_punct,text_no_sw,text_tokens):\n",
    "    wordcount_after_cleaning=0\n",
    "    \n",
    "    #Importing Dictionary for analysis\n",
    "    dic=pd.read_csv(\"Dictionary.csv\")\n",
    "    dic=dic.drop(['Seq_num','Word Count','Word Proportion','Average Proportion','Std Dev','Doc Count', 'Litigious','Strong_Modal','Weak_Modal','Constraining','Source'],axis=1)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    #Counting total number of words after cleaning\n",
    "    for i in text_tokens:\n",
    "        wordcount_after_cleaning+=1\n",
    "    print(\"Done\")\n",
    "    \n",
    "    #Calculating Positive and Negative Scores,syllables per word and complexity of words\n",
    "    positive=0\n",
    "    negative=0\n",
    "    comp=0\n",
    "    syl_count=0\n",
    "    for i in text_tokens:\n",
    "        for j, row in dic.iterrows():\n",
    "            if i == dic.at[j,'Word']:\n",
    "                \n",
    "                #Calculating Syllable count\n",
    "                syl_count = syl_count + dic.at[j,'Syllables']\n",
    "                \n",
    "                #Calculating Positive and Negative Scores\n",
    "                if dic.at[j,'Negative']!=0:\n",
    "                    negative+=1\n",
    "                elif dic.at[j,'Positive']!=0:\n",
    "                    positive+=1\n",
    "                \n",
    "                #Calculating the complexity of the words\n",
    "                if dic.at[j,'Complexity']!=0:\n",
    "                    comp+=1\n",
    "    print(\"Done\")  \n",
    "    #Calculating syllables per word\n",
    "    syl_per_word = syl_count/wordcount_after_cleaning\n",
    "    syl_per_word = syl_per_word.round()\n",
    "    print(\"Done\")\n",
    "    \n",
    "    #Calculating Polarity Score\n",
    "    pol_score = (positive - negative)/((positive + negative) + 0.000001)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    #Calculating Subjectivity Score\n",
    "    sub_score = (positive + negative)/(wordcount_after_cleaning + 0.000001)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    #Calculating total number of words\n",
    "    wc=text.split()\n",
    "    tot_wordcount=len(wc)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    #Calculating total number of sentences\n",
    "    sent_count=text.count('.')+text.count('?')+text.count('!')\n",
    "    print(\"Done\")\n",
    "    \n",
    "    #Calculating Average Sentence Length\n",
    "    avg_sent_len = tot_wordcount/sent_count\n",
    "    print(\"Done\")\n",
    "    \n",
    "    #Calculating Percentage of Complex words\n",
    "    per_of_comp = comp/tot_wordcount\n",
    "    print(\"Done\")\n",
    "    \n",
    "    #Calculating Fog Index\n",
    "    fog_index = 0.4 * (avg_sent_len + per_of_comp)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    #Calculating Average Number of Words per Sentence\n",
    "    avg_words_per_sent = tot_wordcount/sent_count\n",
    "    print(\"Done\")\n",
    "    \n",
    "    \n",
    "    #Calculating number of Personal Pronouns\n",
    "    pos=nltk.pos_tag(text.split())\n",
    "    prp=0\n",
    "    for i in pos:\n",
    "        if i[1]=='PRP':\n",
    "            prp+=1\n",
    "    print(\"Done\")\n",
    "    \n",
    "    #Calculating total number of characters\n",
    "    char_count = 0\n",
    "    for word in text.split():\n",
    "        char_count = char_count + len(word)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    #Calculating Average Word Length\n",
    "    avg_word_len = char_count/tot_wordcount\n",
    "    print(\"Done\")\n",
    "    \n",
    "    #Returning all the required values\n",
    "    return [positive,negative,\n",
    "            pol_score,sub_score,\n",
    "            avg_sent_len,per_of_comp,\n",
    "            fog_index,avg_words_per_sent,\n",
    "            comp,wordcount_after_cleaning,\n",
    "            syl_per_word,prp,avg_word_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ea1851",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1376212",
   "metadata": {},
   "source": [
    "### Basic Operations like stop word removal, punctuation removal, digit removal and tokenization are done here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26beb46f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Initialising Stop Words from NLTK\n",
    "sw_nltk = stopwords.words('english')\n",
    "\n",
    "#Initialising Header for Final Output\n",
    "header=['URL_ID','URL','Positive Score','Negative Score','Polarity Score','Subjectivity Score','Avg Sentence Length','Percentage of Complex Words','Fog Index','Avg Number of Words Per Sentence','Complex Word Count','Word Count','Syllable Per Word','Personal Pronouns','Avg Word Length']\n",
    "\n",
    "#Creating CSV file and inserting header\n",
    "with open('Final_Output.csv','w',newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "\n",
    "#Starting Basic Data Pre-Processing\n",
    "for i,row in df.iterrows():\n",
    "    \n",
    "    #Confirmation of Start\n",
    "    print(str(i)+\" Started\")\n",
    "    \n",
    "    filepath=\"Path to your folder with text files\"+str(i)+\".txt\"\n",
    "    \n",
    "    text=\"\"\n",
    "    \n",
    "    #Reading all text files\n",
    "    with open(filepath,'r') as f: \n",
    "        text = f.read()\n",
    "    \n",
    "    text = text.replace(\"“\", '\"').replace(\"”\",'\"').replace(\"’\",\"'\")\n",
    "    \n",
    "    #Removing Punctuations\n",
    "    text_no_punct= text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    #Removing Stop Words\n",
    "    words = [word for word in tet_no_punct.split() if word.lower() not in sw_nltk]\n",
    "    \n",
    "    text_no_sw= \" \".join(words)\n",
    "    \n",
    "    #Removing Digits\n",
    "    new_text=''.join([i for i in text_no_sw if not i.isdigit()])\n",
    "    \n",
    "    #Change to Upper Class\n",
    "    new_text=new_text.upper()\n",
    "    \n",
    "    #Tokenization\n",
    "    text_tokens = nltk.word_tokenize(new_text)\n",
    "    \n",
    "    #Call FunctionHub function to perform complete analysis\n",
    "    all_functions=FunctionHub(df,text,text_no_punct,text_no_sw,text_tokens)\n",
    "    \n",
    "    #Creating final output csv file\n",
    "    all_functions.insert(0,df['URL'][i])\n",
    "    \n",
    "    output=pd.read_csv(\"Final_Output.csv\",index_col='URL_ID')\n",
    "    \n",
    "    output.loc[len(output.index)]=all_functions\n",
    "    \n",
    "    output.to_csv(\"Final_Output.csv\",index=True)\n",
    "    \n",
    "    #Confirmation of Completion\n",
    "    print(str(i)+\" Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c110ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
